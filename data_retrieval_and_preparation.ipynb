{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "from multiprocess import Pool\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = r\"/nas-slot4/schwurbel2\"\n",
    "\n",
    "def set_path(string):\n",
    "    global path \n",
    "    path = string\n",
    "    \n",
    "\n",
    "def get_channel_names(path):\n",
    "    channel_names = []\n",
    "    for channel in os.listdir(path):\n",
    "        channel_names.append(channel)\n",
    "\n",
    "    return channel_names\n",
    "\n",
    "\n",
    "def run (channel_name):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "\n",
    "    import requests\n",
    "    import re\n",
    "    import os\n",
    "\n",
    "    from multiprocess import Pool\n",
    "    import psutil\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    path = r\"/nas-slot4/schwurbel2\"\n",
    "    #dst = r\"/Users/luka/Documents/Lehrveranstaltungen/Research Seminar/Data/Dataframes\"\n",
    "    dst = \"/data/schwurbelarchiv/extracted_information\" \n",
    "\n",
    "    def get_channel_names(path):\n",
    "        channel_names = []\n",
    "        for channel in os.listdir(path):\n",
    "            channel_names.append(channel)\n",
    "\n",
    "        return channel_names\n",
    "\n",
    "    def find_first_message(content_file):\n",
    "        lines = str(content_file).split(\"\\n\")\n",
    "        mess_text = \"message default clearfix\"\n",
    "        mess_id = \"id=\"\n",
    "        not_wanted = \"message-\"\n",
    "        for i in range(len(lines)):\n",
    "            line = lines[i]\n",
    "            if mess_text in line and mess_id in line and not_wanted not in line:\n",
    "                message_start_id = re.search(r'message(\\d+)', line).group(1)\n",
    "                return int(message_start_id) \n",
    "\n",
    "    def find_last_message(content_file):\n",
    "        lines = str(content_file).split(\"\\n\")\n",
    "        mess_text = \"message default clearfix\"\n",
    "        mess_id = \"id=\"\n",
    "        not_wanted = \"message-\"\n",
    "        for i in range(1,len(lines)):\n",
    "            line = lines[-i]\n",
    "            if mess_text in line and mess_id in line and not_wanted not in line:\n",
    "                print(line)\n",
    "                message_end_id = re.search(r'message(\\d+)', line).group(1)\n",
    "                return int(message_end_id)\n",
    "\n",
    "    def get_content(path):\n",
    "        content = []\n",
    "        # Get the path of each folder\n",
    "        file_path = os.path.join(path, channel_name)\n",
    "        # Check if the path is a directory\n",
    "        if os.path.isdir(file_path):\n",
    "            # Loop through all the files in the folder\n",
    "            for file in os.listdir(file_path):\n",
    "                # Get the path of each file\n",
    "                file_name = os.path.join(file_path, file)\n",
    "                #print(file_name)\n",
    "                # Check if the file name matches the specific one\n",
    "                if file[:7] == 'message':\n",
    "                    # Open the specific file\n",
    "                    with open(file_name, encoding='utf-8') as f:\n",
    "                        print(file_name)\n",
    "                        cont = BeautifulSoup(f, \"html.parser\")\n",
    "                        content_tup = (cont, find_first_message(cont), find_last_message(cont))\n",
    "                        content.append(content_tup)\n",
    "        return content\n",
    "\n",
    "        \n",
    "    def get_author(content):\n",
    "        #create dataframe containing all authors\n",
    "        df_author = pd.DataFrame(columns=['check', 'group_name', 'posting_date', 'author', 'fwd_author' ])\n",
    "\n",
    "        pattern = r'title=\"(.*?)\"'\n",
    "        for n in range(len(content)):\n",
    "\n",
    "            for i in range(content[n][1], content[n][2]):\n",
    "                try:\n",
    "                    if \"forwarded body\" in str(content[n][0].find('div', id = \"message%s\"%i)):\n",
    "                        df_author = pd.concat([df_author, pd.DataFrame([[i, content[n][0].find('div', class_= \"text bold\").string.strip(), re.search(pattern, str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"body\"))).group(1), hash(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"from_name\").string.strip()), hash(re.search(r'(?<=from_name\">\\n).*(?= <span)', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"forwarded body\").find('div', class_=\"from_name\")).strip()).group())]], columns=['check', 'group_name', 'posting_date', 'author', 'fwd_author'])])\n",
    "\n",
    "                    else: \n",
    "                        df_author = pd.concat([df_author, pd.DataFrame([[i, content[n][0].find('div', class_= \"text bold\").string.strip(), re.search(pattern, str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"body\"))).group(1), hash(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"from_name\").string.strip())]], columns=['check','group_name', 'posting_date', 'author'])])\n",
    " \n",
    "                except:\n",
    "                    continue\n",
    "        return df_author\n",
    "\n",
    "\n",
    "\n",
    "    def get_message(content):\n",
    "        #create dataframe with all messages\n",
    "        df_message = pd.DataFrame(columns=['check', 'posting_date', 'message', 'fwd_message', 'fwd_posting_date'])\n",
    "        pattern = r'title=\"(.*?)\"'\n",
    "\n",
    "\n",
    "        for n in range(len(content)):\n",
    "\n",
    "            #pattern to detect text\n",
    "            text_pattern = re.compile(r'<div class=\"text\">(.*?)</div>', re.DOTALL)\n",
    "\n",
    "            \n",
    "            for i in range(content[n][1], content[n][2]):\n",
    "                try:\n",
    "                    if \"forwarded body\" in str(content[n][0].find('div', id = \"message%s\"%i)):\n",
    "                        text_content = re.findall(text_pattern, str(content[n][0].find('div', id = \"message%s\"%i)))[0]\n",
    "                        df_message = pd.concat([df_message, pd.DataFrame([[i, re.search(pattern, str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"body\"))).group(1), text_content.replace('<br/>', ' ').replace('\\n', ''), re.search(r'<span class=\"details\">(.*?)</span>', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"forwarded body\").find('div', class_=\"from_name\")).strip()).group(1)]], columns=['check', 'posting_date', 'fwd_message', 'fwd_posting_date'])])\n",
    "                    else: \n",
    "                        text_content = re.findall(text_pattern, str(content[n][0].find('div', id = \"message%s\"%i)))[0]\n",
    "                        df_message = pd.concat([df_message, pd.DataFrame([[i, re.search(pattern, str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"body\"))).group(1), text_content.replace('<br/>', ' ').replace('\\n', '')]], columns=['check', 'posting_date', 'message'])])\n",
    "                except:\n",
    "                    continue\n",
    "        return df_message\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_file(content):\n",
    "        #create dataframe containing all media files\n",
    "        df_file = pd.DataFrame(columns=['check', 'posting_date', 'media_file', 'media_file_type', 'fwd_media_file', 'fwd_media_file_type', 'fwd_posting_date'])\n",
    "\n",
    "\n",
    "        pattern = r'title=\"(.*?)\"'\n",
    "\n",
    "        for n in range(len(content)):\n",
    "\n",
    "\n",
    "            for i in range(content[n][1], content[n][2]):\n",
    "                try:\n",
    "                    if \"forwarded body\" in str(content[n][0].find('div', id = \"message%s\"%i)):\n",
    "                        if \"video\" in str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\")):\n",
    "                            df_file = pd.concat([df_file, pd.DataFrame([[i, re.search(pattern, str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"body\"))).group(1), re.search('href=\"(.*)\"', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\"))).group(1), 'video', re.search(r'<span class=\"details\">(.*?)</span>', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"forwarded body\").find('div', class_=\"from_name\")).strip()).group(1)]], columns=['check', 'posting_date', 'fwd_media_file', 'fwd_media_file_type', 'fwd_posting_date'])])\n",
    "\n",
    "                        elif \"photo\" in str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\")):\n",
    "                            df_file = pd.concat([df_file, pd.DataFrame([[i, re.search(pattern, str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"body\"))).group(1), re.search('href=\"(.*)\"', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\"))).group(1), 'photo', re.search(r'<span class=\"details\">(.*?)</span>', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"forwarded body\").find('div', class_=\"from_name\")).strip()).group(1)]], columns=['check', 'posting_date', 'fwd_media_file', 'fwd_media_file_type', 'fwd_posting_date'])])\n",
    "\n",
    "                        elif \"Voice message\" in str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\")):\n",
    "                            df_file = pd.concat([df_file, pd.DataFrame([[i, re.search(pattern, str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"body\"))).group(1), re.search('href=\"(.*)\"', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\"))).group(1), 'voice message', re.search(r'<span class=\"details\">(.*?)</span>', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"forwarded body\").find('div', class_=\"from_name\")).strip()).group(1)]], columns=['check', 'posting_date', 'fwd_media_file', 'fwd_media_file_type', 'fwd_posting_date'])])\n",
    "\n",
    "                        else: \n",
    "\n",
    "                            continue    \n",
    "    \n",
    "                    else:\n",
    "                        if \"video\" in str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\")):\n",
    "                            df_file = pd.concat([df_file, pd.DataFrame([[i, re.search(pattern, str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"body\"))).group(1), re.search('href=\"(.*)\"', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\"))).group(1), 'video']], columns=['check', 'posting_date', 'media_file', 'media_file_type'])])\n",
    "\n",
    "                        elif \"photo\" in str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\")):\n",
    "                            df_file = pd.concat([df_file, pd.DataFrame([[i, re.search(pattern, str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"body\"))).group(1), re.search('href=\"(.*)\"', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\"))).group(1), 'photo']], columns=['check', 'posting_date', 'media_file', 'media_file_type'])])\n",
    "\n",
    "                        elif \"Voice message\" in str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\")):\n",
    "                            df_file = pd.concat([df_file, pd.DataFrame([[i, re.search(pattern, str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"body\"))).group(1), re.search('href=\"(.*)\"', str(content[n][0].find('div', id = \"message%s\"%i).find('div', class_=\"media_wrap clearfix\"))).group(1), 'voice message']], columns=['check', 'posting_date', 'media_file', 'media_file_type'])])\n",
    "\n",
    "                        else:\n",
    "  \n",
    "                            continue\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        return df_file\n",
    "\n",
    "    def get_dataset(content):\n",
    "        #joining step by step with different kind of joins to ensure data reliability        \n",
    "        df_content = pd.merge(get_message(content), get_file(content), on = 'posting_date', how = 'outer')\n",
    "        df_merge = pd.merge(df_content, get_author(content), on = 'posting_date', how = 'outer')\n",
    "        df_merge['group_name'], df_merge['author'] = df_merge['group_name'].ffill(), df_merge['author'].ffill()\n",
    "        df_merge['posting_date'] = pd.to_datetime(df_merge['posting_date'], dayfirst = True, utc=True)\n",
    "        df_merge[\"week\"] = df_merge[\"posting_date\"].dt.weekofyear\n",
    "        df_merge[\"day\"] = df_merge[\"posting_date\"].dt.floor('D') + pd.Timedelta(12, unit='h')\n",
    "        df_merge[\"weekday\"] = df_merge[\"posting_date\"].dt.weekday\n",
    "        df_merge['message_hash'] = df_merge['message'][df_merge['message'].notna()].apply(hash)\n",
    "        df_merge['fwd_message_hash'] = df_merge['fwd_message'][df_merge['fwd_message'].notna()].apply(hash)\n",
    "\n",
    "        return df_merge\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    channel_list = get_channel_names(r\"/nas-slot4/schwurbel2\")\n",
    "\n",
    "\n",
    "    get_dataset(get_content(path)).to_csv(\n",
    "\t\tPath(dst, channel_name + \".csv.gzip\"),\n",
    "\t\t index=False,\n",
    "\t\t  compression=\"gzip\"\n",
    "\t)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "number_of_cores = 100\n",
    "\n",
    "# this opens a pool of cores that is then used to compute jobs for you\n",
    "pool = Pool(number_of_cores)\n",
    "\n",
    "# get a list of all channels you want to process (fill this in with\n",
    "# actual working code to get the channel names).\n",
    "channel_list = get_channel_names(r\"/nas-slot4/schwurbel2\")\n",
    "\n",
    "# iterates over the list of channels and runs the function run()\n",
    "# on one core for each channel. Wrapping the call to pool.imap_unordered()\n",
    "# in tqdm() gives you a nice progress bar for the process.\n",
    "'''\n",
    "for channel in tqdm(\n",
    "\tpool.imap_unordered(func=run, iterable=channel_list),\n",
    "\ttotal=len(channel_list)\n",
    "\t):\n",
    "\t\t# note: the \"run\" function already saves the results to\n",
    "\t\t# disk, so you don't need to do anything here. Alternatively,\n",
    "\t\t# you could return the individual channel's df from run() and\n",
    "\t\t# concatenate all the extracted data here\n",
    "\t\tpass\n",
    "        \n",
    "'''\n",
    "\n",
    "# it is important to close the pool of processes after completing all jobs\n",
    "pool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"/data/schwurbelarchiv/extracted_information\"\n",
    "folder_path = '/data/schwurbelarchiv/extracted_information/*.csv.gzip'\n",
    "\n",
    "all_files = glob.glob(folder_path)\n",
    "\n",
    "beginning = \"/data/schwurbelarchiv/extracted_information/\"\n",
    "end = \".csv.gzip\"\n",
    "\n",
    "\n",
    "    \n",
    "folder_dict = {}\n",
    "\n",
    "\n",
    "base_df = pd.read_csv(all_files[0], compression=\"gzip\", low_memory = False)\n",
    "file = all_files[0][len(beginning):-len(end)]\n",
    "        #print(tmp['group_name'].unique())\n",
    "        #if len(tmp['group_name'].unique()) != 0:\n",
    "try:\n",
    "    gname = tmp['group_name'].unique()[0]\n",
    "    folder_dict[gname]=file\n",
    "       \n",
    "except:\n",
    "    pass\n",
    "\n",
    "count = 0\n",
    "\n",
    "for f in all_files[1:]:\n",
    "    try:\n",
    "        tmp = pd.read_csv(f, compression=\"gzip\", low_memory = False)\n",
    "        file = f[len(beginning):-len(end)]\n",
    "        #print(tmp['group_name'].unique())\n",
    "        #if len(tmp['group_name'].unique()) != 0:\n",
    "        gname = tmp['group_name'].unique()[0]\n",
    "        folder_dict[gname]=file\n",
    "        base_df = pd.concat([base_df, tmp], axis = 0)\n",
    "        count += 1\n",
    "        print(count)\n",
    "    except:\n",
    "        count += 1\n",
    "        continue\n",
    "\n",
    "base_df[\"transcribed_message\"] = np.nan\n",
    "base_df = base_df.reset_index(drop=True)\n",
    "base_df.to_csv(\"/data/schwurbelarchiv/extracted_information/dataframes/base_df_alt.csv.gzip\", compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_df = pd.read_csv(\"/data/schwurbelarchiv/extracted_information/dataframes/base_df.csv.gzip\", compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = base_df[base_df['group_name'] != 'LichtkriegerForumChat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = base_df[base_df['group_name'] != '🛡 Bismarcks Erben']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = base_df[base_df['group_name'] != 'Covid-Impftod Corona Impfungen Nebenwirkungen - Wirksamkeit - Notwendigkeit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = base_df[base_df['group_name'] != '🤹🏻‍♂️Alle Punkte Verbunden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = base_df[base_df['group_name'] != 'KlarTraum-Gruppe Offenlegungen und Aufwachen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = base_df[base_df['group_name'] != 'Freiheit ist ein Recht - Kanal !!!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df['posting_date'] = pd.to_datetime(base_df['posting_date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%d-%m %H:%M:%S') ##one time run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df['posting_date'] = pd.to_datetime(base_df['posting_date'], dayfirst=True, utc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df[\"week\"] = base_df[\"posting_date\"].dt.weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df['day'] = base_df['posting_date'].dt.floor('D') + pd.Timedelta(12, unit = 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df[\"weekday\"] = base_df[\"posting_date\"].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_group(n_authors):\n",
    "    if n_authors > 1:\n",
    "        return \"group\"\n",
    "    else:\n",
    "        return \"channel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info = base_df.groupby(['group_name'])[\"author\"].nunique().reset_index(name=\"n_authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info[\"is_group\"] = group_info[\"n_authors\"].apply(is_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info['n_messages_total'] = group_info['group_name'].map(base_df[\"group_name\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = base_df.groupby([\"group_name\"])[\"message\"].count().reset_index(name=\"n_text_messages\")\n",
    "group_info = pd.merge(group_info, new_df, on='group_name')\n",
    "new_df = base_df.groupby([\"group_name\"])[\"fwd_message\"].count().reset_index(name=\"n_fwd_text_messages\")\n",
    "group_info = pd.merge(group_info, new_df, on='group_name')\n",
    "new_df = base_df.groupby([\"group_name\"])[\"media_file\"].count().reset_index(name=\"n_media\")\n",
    "group_info = pd.merge(group_info, new_df, on='group_name')\n",
    "new_df = base_df.groupby([\"group_name\"])[\"fwd_media_file\"].count().reset_index(name=\"n_fwd_media\")\n",
    "group_info = pd.merge(group_info, new_df, on='group_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = base_df.groupby('group_name')['media_file_type'].apply(lambda x: x[x == \"photo\"].count()).to_frame()\n",
    "new_df = new_df.rename(columns={\"media_file_type\": \"photos\"})\n",
    "new_df[\"videos\"] = base_df.groupby('group_name')['media_file_type'].apply(lambda x: x[x == \"video\"].count()).to_frame()\n",
    "new_df[\"voice messages\"] = base_df.groupby('group_name')['media_file_type'].apply(lambda x: x[x == \"voice message\"].count()).to_frame()\n",
    "new_df[\"fwd_photos\"] = base_df.groupby('group_name')['fwd_media_file_type'].apply(lambda x: x[x == \"photo\"].count()).to_frame()\n",
    "new_df[\"fwd_videos\"] = base_df.groupby('group_name')['fwd_media_file_type'].apply(lambda x: x[x == \"video\"].count()).to_frame()\n",
    "new_df[\"fwd_voice messages\"] = base_df.groupby('group_name')['fwd_media_file_type'].apply(lambda x: x[x == \"voice message\"].count()).to_frame()\n",
    "group_info = pd.merge(group_info, new_df, on='group_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info[\"total_media\"] = group_info[\"n_media\"] + group_info[\"n_fwd_media\"]\n",
    "group_info[\"total_photos\"] = group_info[\"photos\"] + group_info[\"fwd_photos\"]\n",
    "group_info[\"total_videos\"] = group_info[\"videos\"] + group_info[\"fwd_videos\"]\n",
    "group_info[\"total_voice_m\"] = group_info[\"voice messages\"] + group_info[\"fwd_voice messages\"]\n",
    "group_info[\"media_ratio\"] = group_info[\"total_media\"]/group_info[\"n_messages_total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame({\"group_name\":[\"total\"] ,\n",
    "                           \"n_authors\":[group_info[\"n_authors\"].sum()],\n",
    "                           \"is_group\": [str(group_info[\"is_group\"].value_counts()[\"channel\"])\n",
    "                                        + \" channels, \" \n",
    "                                        + str(group_info[\"is_group\"].value_counts()[\"group\"]) \n",
    "                                        + \" groups\"],\n",
    "                           \"n_messages_total\": [group_info[\"n_messages_total\"].sum()], \n",
    "                           \"n_text_messages\": [group_info[\"n_text_messages\"].sum()], \n",
    "                           \"n_fwd_text_messages\": [group_info[\"n_fwd_text_messages\"].sum()],\n",
    "                           \"n_media\": [group_info[\"n_media\"].sum()],\n",
    "                           \"n_fwd_media\": [group_info[\"n_fwd_media\"].sum()],\n",
    "                           \"photos\": [group_info[\"photos\"].sum()],\n",
    "                           \"videos\": [group_info[\"videos\"].sum()],\n",
    "                           \"voice messages\": [group_info[\"voice messages\"].sum()],\n",
    "                           \"fwd_photos\": [group_info[\"fwd_photos\"].sum()],\n",
    "                           \"fwd_videos\": [group_info[\"fwd_videos\"].sum()],\n",
    "                           \"fwd_voice messages\": [group_info[\"fwd_voice messages\"].sum()],\n",
    "                           \"total_media\": [group_info[\"total_media\"].sum()],\n",
    "                           \"total_photos\": [group_info[\"total_photos\"].sum()],\n",
    "                           \"total_videos\": [group_info[\"total_videos\"].sum()],\n",
    "                           \"total_voice_m\": [group_info[\"total_voice_m\"].sum()],\n",
    "                           \"media_ratio\": [group_info[\"total_media\"].sum()/group_info[\"n_messages_total\"].sum()] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info = pd.concat([group_info, summary_df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_info = group_info.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_info.to_csv(\"/data/schwurbelarchiv/extracted_information/dataframes/group_info.csv.gzip\", compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
